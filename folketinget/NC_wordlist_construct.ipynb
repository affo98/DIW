{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmeeting = pd.read_parquet('./data/data_meeting.parquet')\n",
    "data_agenda1 = pd.read_parquet('./data/data_agenda1.parquet')\n",
    "data_agenda2 = pd.read_parquet('./data/data_agenda2.parquet')\n",
    "data_agenda3 = pd.read_parquet('./data/data_agenda3.parquet')\n",
    "data_speech1 = pd.read_parquet('./data/data_speech1.parquet')\n",
    "data_speech2 = pd.read_parquet('./data/data_speech2.parquet')\n",
    "data_speech3 = pd.read_parquet('./data/data_speech3.parquet')\n",
    "parMem = pd.read_parquet('./data/parliament_members.parquet')\n",
    "\n",
    "dagenda = pd.concat([data_agenda1, data_agenda2, data_agenda3], axis=0)\n",
    "dspeech = pd.concat([data_speech1, data_speech2, data_speech3], axis=0)\n",
    "\n",
    "annotation_data = pd.read_csv('data/annotation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agendaC = annotation_data[annotation_data['label'] == 'C']\n",
    "speechC = dspeech[dspeech[['meeting_id', 'agenda_item_id']].apply(tuple, axis=1).isin(agendaC[['meeting_id', 'agenda_item_id']].apply(tuple, axis=1))]\n",
    "speechC['label'] = 'C'\n",
    "agendaNC = annotation_data[annotation_data['label'] == 'NC']\n",
    "speechNC = dspeech[dspeech[['meeting_id', 'agenda_item_id']].apply(tuple, axis=1).isin(agendaNC[['meeting_id', 'agenda_item_id']].apply(tuple, axis=1))]\n",
    "speechNC = pd.merge(speechNC, agendaNC[['meeting_id', 'agenda_item_id', 'group']], on=['meeting_id', 'agenda_item_id'], how='left')\n",
    "speechNC['label'] = 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "exception_list = ['CO2', 'co2']\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = [token.text.lower()for token in doc if not token.is_stop and token.is_alpha or token.text in exception_list]\n",
    "    return processed_tokens\n",
    "\n",
    "def preprocess_text_lemma(text):\n",
    "    doc = nlp(text)\n",
    "    processed_tokens_lemma = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha or token.text in exception_list]\n",
    "    return processed_tokens_lemma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_comb = pd.concat([speechC, speechNC])\n",
    "speech_comb = speech_comb.rename(columns={'label': 'label_agenda'})\n",
    "speech_comb['processed_tokens'] = speech_comb.speech_item_text.apply(preprocess_text)\n",
    "speech_comb['processed_tokens_lemma'] = speech_comb.speech_item_text.apply(preprocess_text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = speech_comb[[\"meeting_id\",\"agenda_item_id\",\"speech_item_id\",\"label_agenda\",\"processed_tokens\",\"processed_tokens_lemma\"]]\n",
    "df['id'] = df[\"meeting_id\"].astype(str) + \"_\" + df[\"agenda_item_id\"].astype(str) + \"_\" + df[\"speech_item_id\"].astype(str)\n",
    "df = df.drop(columns=[\"meeting_id\",\"agenda_item_id\",\"speech_item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../labeled_tokenized_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Odds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caluc_proportion(numerator, denominator):\n",
    "    if denominator == 0:\n",
    "        if numerator == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return numerator\n",
    "    else:\n",
    "        if numerator == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "unique_words_list_C = []\n",
    "unique_words_list_NC = []\n",
    "\n",
    "for i in range(len(df[df['label_agenda']=='C'])):\n",
    "    unique_words_list_C.extend(df[df['label_agenda']=='C'].processed_tokens_lemma.iloc[i])\n",
    "for i in range(len(df[df['label_agenda']=='NC'])):\n",
    "    unique_words_list_NC.extend(df[df['label_agenda']=='NC'].processed_tokens_lemma.iloc[i])\n",
    "\n",
    "unique_words_C_counter = Counter(unique_words_list_C)\n",
    "unique_words_C_df= pd.DataFrame.from_dict(unique_words_C_counter, orient='index', columns = ['word_count_C'])\n",
    "unique_words_C_df.reset_index(inplace=True)\n",
    "unique_words_C_df.rename(columns={'index': 'word'}, inplace=True)\n",
    "\n",
    "unique_words_NC_counter = Counter(unique_words_list_NC)\n",
    "unique_words_NC_df= pd.DataFrame.from_dict(unique_words_NC_counter, orient='index', columns = ['word_count_NC'])\n",
    "unique_words_NC_df.reset_index(inplace=True)\n",
    "unique_words_NC_df.rename(columns={'index': 'word'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = pd.merge(unique_words_C_df, unique_words_NC_df, on='word', how='outer')\n",
    "unique_words[['word_count_C', 'word_count_NC']] = unique_words[['word_count_C', 'word_count_NC']].fillna(0)\n",
    "unique_words['frequency_C'] = unique_words['word_count_C'] / sum(unique_words_C_df['word_count_C'])\n",
    "unique_words['frequency_NC'] = unique_words['word_count_NC'] / sum(unique_words_NC_df['word_count_NC'])\n",
    "unique_words[\"odds_C\"] = unique_words.apply(lambda row: caluc_proportion(row['frequency_C'], row['frequency_NC']), axis=1)\n",
    "unique_words[\"odds_NC\"] = unique_words.apply(lambda row: caluc_proportion(row['frequency_NC'], row['frequency_C']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diw_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
