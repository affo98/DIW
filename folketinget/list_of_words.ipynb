{"cells":[{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":10449,"status":"ok","timestamp":1701600941027,"user":{"displayName":"e okuda","userId":"05819132607139876861"},"user_tz":-60},"id":"zvLEQfr-Izy8"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from gensim.models import KeyedVectors\n","from adjustText import adjust_text\n","import multiprocessing as mp\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import pyarrow as pa\n","import pyarrow.parquet as pq"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["%run helper_functions"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23720,"status":"ok","timestamp":1701600964737,"user":{"displayName":"e okuda","userId":"05819132607139876861"},"user_tz":-60},"id":"XbZrsIZPLu3M","outputId":"2a2ae5f3-764f-4dec-d007-1f4f9fdc816f"},"outputs":[],"source":["data_path = \"./data/\""]},{"cell_type":"code","execution_count":30,"metadata":{"id":"fLpWrX-EMQ51"},"outputs":[],"source":["annotation_data = pd.read_csv(data_path + 'annotation_data.csv')"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["df_tok1 = pd.read_parquet('./data/data_speech1_tok.parquet')\n","df_tok2 = pd.read_parquet('./data/data_speech2_tok.parquet')\n","df = pd.concat([df_tok1, df_tok2]).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["Choose speech items from annotated agenda items"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["anno_tokens = pd.merge(df, annotation_data[[\"meeting_id\",\"agenda_item_id\",\"label\"]], on=[\"meeting_id\",\"agenda_item_id\"])\n","anno_tokens.rename(columns={'label':'label_agenda',\"tokenized_speech\":\"speech_item_tokenized\"},inplace=True)\n","anno_tokens[\"speech_item_tokenized\"] = anno_tokens[\"speech_item_tokenized\"].apply(lambda x: eval(x))"]},{"cell_type":"markdown","metadata":{"id":"c8hZoMnCIzzC"},"source":["### Calculate Odds\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1539,"status":"ok","timestamp":1701600995339,"user":{"displayName":"e okuda","userId":"05819132607139876861"},"user_tz":-60},"id":"rZm97sGAs1V7"},"outputs":[],"source":["df_C = anno_tokens[anno_tokens['label_agenda']=='C']\n","df_NC = anno_tokens[anno_tokens['label_agenda']=='NC']\n","\n","unique_words_C_list = [word for tokens in df_C.speech_item_tokenized for word in tokens]\n","unique_words_NC_list = [word for tokens in df_NC.speech_item_tokenized for word in tokens]\n","\n","unique_words_C_dict = Counter(unique_words_C_list)\n","unique_words_NC_dict = Counter(unique_words_NC_list)\n","\n","unique_words_C_df = pd.DataFrame.from_dict(unique_words_C_dict, orient='index', columns=['word_count_C']).reset_index().rename(columns={'index': 'word'})\n","unique_words_NC_df = pd.DataFrame.from_dict(unique_words_NC_dict, orient='index', columns=['word_count_NC']).reset_index().rename(columns={'index': 'word'})\n","\n","#merge\n","unique_words = pd.merge(unique_words_C_df, unique_words_NC_df, on='word', how='outer')\n","unique_words[['word_count_C', 'word_count_NC']] = unique_words[['word_count_C', 'word_count_NC']].fillna(0)\n","unique_words['frequency_C'] = unique_words['word_count_C'] / sum(unique_words_C_df['word_count_C'])\n","unique_words['frequency_NC'] = unique_words['word_count_NC'] / sum(unique_words_NC_df['word_count_NC'])\n","freq_1_NC = unique_words[unique_words['word_count_NC']==1]['frequency_NC'].iloc[0]\n","freq_1_C = unique_words[unique_words['word_count_C']==1]['frequency_C'].iloc[0]\n","\n","unique_words[\"odds_C\"] = unique_words.apply(lambda row: calculate_odds(row['frequency_C'], row['frequency_NC'], 'C', freq_1_NC), axis=1)\n","unique_words[\"odds_NC\"] = unique_words.apply(lambda row: calculate_odds(row['frequency_NC'], row['frequency_C'], 'NC', freq_1_C), axis=1)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Make Percentiles"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_words['rank_odds_C'] = unique_words['odds_C'].rank(ascending=False)\n","unique_words['rank_odds_NC'] = unique_words['odds_NC'].rank(ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_words_above_1_C = unique_words[(unique_words['odds_C'] > 1) & (unique_words['frequency_C'] > 0.0001)]\n","                          \n","unique_words_above_1_C['rank'] = unique_words_above_1_C['odds_C'].rank(method='first')\n","unique_words_above_1_C['odds_C_percentile'] = unique_words_above_1_C['rank'].apply(lambda x: map_rank_to_bin(unique_words_above_1_C, x))\n","\n","unique_words_above_1_NC = unique_words[(unique_words['odds_NC'] > 1) & (unique_words['frequency_NC'] > 0.0001)]\n","unique_words_above_1_NC['rank'] = unique_words_above_1_NC['odds_NC'].rank(method='first')\n","unique_words_above_1_NC['odds_NC_percentile'] = unique_words_above_1_NC['rank'].apply(lambda x: map_rank_to_bin(unique_words_above_1_NC, x))"]},{"cell_type":"markdown","metadata":{},"source":["## Adding DSL word-to-vec, choosing Odds, k-means, and cosine similarities"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# download the model from here: https://korpus.dsl.dk/resources/licences/dsl-open.html#en\n","MODEL_FILE = '~/Desktop/dsl_skipgram_2020_m5_f500_epoch2_w5.model.w2v.bin'\n","word_vec_model = KeyedVectors.load_word2vec_format(MODEL_FILE, binary=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_percentiles = pd.DataFrame(columns=['percentile', 'perplexity', 'prop_c_not_in_group_c', 'prop_nc_not_in_group_nc', 'cosine_similarities_C', 'cosine_similarities_NC'])\n","\n","# generate different dataframes with different odds\n","\n","for percentile in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]:\n","    data = choose_odds_percentiles(percentile,unique_words_above_1_C,unique_words_above_1_NC,word_vec_model)\n","    \n","    data = k_means(data, 2, perplex=20)\n","    k_means_group_c = data[data['label']=='C']['k_means_group_tsne'].value_counts().idxmax()\n","    data['k_means_group_tsne'] = np.where(data['k_means_group_tsne'] == k_means_group_c, 'C', 'NC')\n","    prop_c_not_in_group_c = len(data[(data['label']=='C') & (data['k_means_group_tsne'] == 'NC')]) / len(data[data['label']=='C'])\n","    prop_nc_not_in_group_nc = len(data[(data['label']=='NC') & (data['k_means_group_tsne'] == 'C')]) / len(data[data['label']=='NC'])\n","\n","    cosine_similarities_C = cosine_similarities(data, 'C', word_vec_model)\n","    cosine_similarities_NC = cosine_similarities(data, 'NC', word_vec_model)\n","\n","    data_df = pd.DataFrame({\n","        'percentile': [percentile],\n","        'perplexity': [20],\n","        'prop_c_not_in_group_c': [prop_c_not_in_group_c],\n","        'prop_nc_not_in_group_nc': [prop_nc_not_in_group_nc],\n","        'cosine_similarities_C': [cosine_similarities_C],\n","        'cosine_similarities_NC': [cosine_similarities_NC]\n","    })\n","\n","    df_percentiles = pd.concat([df_percentiles, data_df], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(df_percentiles['percentile'], df_percentiles['cosine_similarities_C'], marker='o', label='C', color = 'blue')\n","plt.plot(df_percentiles['percentile'], df_percentiles['cosine_similarities_NC'], marker='o', label='NC', color='red')\n","plt.legend()\n","\n","plt.xlabel('Odds percentile')\n","plt.ylabel('Cosine Similarity')\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_k_means_C(df):\n","    plt.figure(figsize=(16, 12))\n","    df = df[df['label']=='C']\n","    df['label_numeric'] = pd.Categorical(df['classification']).codes\n","    colors = ['b', 'c', 'y', 'm']\n","    scatter_TP = plt.scatter(x=df[df['classification']=='TP']['embedded_data1'], y=df[df['classification']=='TP']['embedded_data2'], marker='o', color=colors[0], s = 4)\n","    scatter_FN = plt.scatter(x=df[df['classification']=='FN']['embedded_data1'], y=df[df['classification']=='FN']['embedded_data2'], marker='x', color=colors[3], s = 4)\n","    \n","    texts = [plt.text(row['embedded_data1'], row['embedded_data2'], row['word'], fontsize=6, ha='right') for i, row in df.iterrows()]\n","\n","    adjust_text(texts, arrowprops=dict(arrowstyle='-', color='black'), force_text=0.5, force_points=0.5)\n","    \n","    plt.legend((scatter_TP, scatter_FN),\n","           ('True positive', 'False negative'),\n","           loc='upper right',\n","           fontsize=12)\n","    \n","    plt.savefig('plotC.svg', format='svg')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["percentile_85 = pd.concat([choose_odds_percentiles(18,unique_words_above_1_C,unique_words_above_1_NC,word_vec_model),\n","                           choose_odds_percentiles(19,unique_words_above_1_C,unique_words_above_1_NC,word_vec_model), \n","                           choose_odds_percentiles(20,unique_words_above_1_C,unique_words_above_1_NC,word_vec_model)])\n","percentile_85 = k_means(percentile_85, 2, perplex=20)\n","k_means_group_c = percentile_85[percentile_85['label']=='C']['k_means_group_tsne'].value_counts().idxmax()\n","percentile_85['k_means_group_tsne'] = np.where(percentile_85['k_means_group_tsne'] == k_means_group_c, 'C', 'NC')\n","\n","percentile_85['classification'] = np.where(\n","    (percentile_85['label'] == 'C') & (percentile_85['k_means_group_tsne'] == 'C'), 'TP',\n","    np.where((percentile_85['label'] == 'NC') & (percentile_85['k_means_group_tsne'] == 'NC'), 'TN',\n","             np.where((percentile_85['label'] == 'C') & (percentile_85['k_means_group_tsne'] == 'NC'), 'FN', 'FP')))\n","\n","plot_k_means_C(percentile_85)"]},{"cell_type":"markdown","metadata":{},"source":["### Manually check wordlists"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#use 95 percentiles to find shared common words\n","data_C = unique_words.loc[unique_words[\"word_count_C\"] > 0, 'frequency_C']\n","percentile_95_C = np.percentile(data_C, 95)\n","\n","data_NC = unique_words.loc[unique_words[\"word_count_NC\"] > 0, 'frequency_NC']\n","percentile_95_NC = np.percentile(data_NC, 95)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generic_words_list = unique_words[(unique_words.frequency_C>percentile_95_C)&(unique_words.frequency_NC>percentile_95_NC)& \n","                                  (unique_words.odds_C > 0.9) & (unique_words.odds_C < 1.1) &\n","                                  (unique_words.odds_NC > 0.9) & (unique_words.odds_NC < 1.1)].word.to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# with open('C_TP.txt', 'w',  encoding='utf-8') as file:\n","#     for item in percentile_85[percentile_85['classification']=='TP']['word'].tolist():\n","#         file.write(str(item) + '\\n')\n","        \n","    \n","# with open('C_FN.txt', 'w', encoding='utf-8') as file:\n","#     for item in percentile_85[percentile_85['classification']=='FN']['word'].tolist():\n","#         file.write(str(item) + '\\n')\n","\n","# with open('NC.txt', 'w', encoding='utf-8') as file:\n","#     for item in percentile_85[percentile_85['label']=='NC']['word'].tolist():\n","#         file.write(str(item) + '\\n')      \n","\n","# with open('./data/word_lists/word_list_generic.txt', 'w', encoding='utf-8') as file:\n","#     for item in generic_words_list:\n","#         file.write(str(item) + '\\n')\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Add odds to words"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_list_C = pd.read_csv('C_TP.txt', index_col = None)\n","word_list_C = pd.merge(word_list_C, percentile_85[['word', 'odds_C']], on = 'word', how = 'left')\n","#word_list_C.to_parquet('./data/word_lists/word_list_C.parquet')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_list_NC = pd.read_csv('NC.txt', index_col = None)\n","word_list_NC = pd.merge(word_list_NC, percentile_85[['word', 'odds_NC']], on = 'word', how = 'left')\n","#word_list_NC.to_parquet('./data/word_lists/word_list_NC.parquet')"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
