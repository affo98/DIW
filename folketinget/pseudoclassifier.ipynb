{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmeeting = pd.read_parquet('./data/data_meeting.parquet')\n",
    "data_agenda1 = pd.read_parquet('./data/data_agenda1.parquet')\n",
    "data_agenda2 = pd.read_parquet('./data/data_agenda2.parquet')\n",
    "data_agenda3 = pd.read_parquet('./data/data_agenda3.parquet')\n",
    "data_speech1 = pd.read_parquet('./data/data_speech1.parquet')\n",
    "data_speech2 = pd.read_parquet('./data/data_speech2.parquet')\n",
    "data_speech3 = pd.read_parquet('./data/data_speech3.parquet')\n",
    "parMem = pd.read_parquet('./data/parliament_members.parquet')\n",
    "\n",
    "dagenda = pd.concat([data_agenda1, data_agenda2, data_agenda3], axis=0)\n",
    "dspeech = pd.concat([data_speech1, data_speech2, data_speech3], axis=0)\n",
    "\n",
    "annotation_data = pd.read_csv('data/annotation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_data[\"id\"] = annotation_data[\"meeting_id\"].astype(str) + \"_\" + annotation_data[\"agenda_item_id\"].astype(str)\n",
    "dspeech[\"id\"] = dspeech[\"meeting_id\"].astype(str) + \"_\" + dspeech[\"agenda_item_id\"].astype(str)\n",
    "dspeech = dspeech.loc[~dspeech.id.isin(annotation_data.id)].reset_index(drop=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_FILE = '../../dsl_skipgram_2020_m5_f500_epoch2_w5.model.w2v.bin'\n",
    "model = KeyedVectors.load_word2vec_format(MODEL_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text= []\n",
    "s_i_embeddings = []\n",
    "for speech_item in tqdm(dspeech[\"speech_item_text\"].to_list()):\n",
    "    tokenized, s_i_e = preprocess_text(speech_item)\n",
    "    tokenized_text.append(tokenized)\n",
    "    s_i_embeddings.append(s_i_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspeech[\"tokenized_text\"] = tokenized_text\n",
    "dspeech[\"s_i_embeddings\"] = s_i_embeddings\n",
    "\n",
    "dspeech.to_parquet('data/data_speech_tokenized.parquet', compression='gzip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/C_word_list.txt', 'r', encoding='utf-8') as f:\n",
    "    C_words_list = f.read().splitlines()\n",
    "    C_words_set = set(C_words_list)\n",
    "\n",
    "with open('data/NC_word_list.txt', 'r', encoding='utf-8') as f:\n",
    "    NC_words_list = f.read().splitlines()\n",
    "    NC_words_set = set(NC_words_list)\n",
    "\n",
    "with open('data/NC_word_list_all.txt', 'r', encoding='utf-8') as f:\n",
    "    NC_words_all = f.read().splitlines()\n",
    "    NC_words_set_all = set(NC_words_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = []\n",
    "for speech_item in tqdm(dspeech_sample['tokenized_text'].to_list()):\n",
    "    C_count = 0\n",
    "    NC_count = 0\n",
    "    NC_count_all = 0\n",
    "    for word in speech_item:\n",
    "        if word in C_words_set:\n",
    "            C_count += 1\n",
    "        elif word in NC_words_set:\n",
    "            NC_count += 1\n",
    "        elif word in NC_words_set_all:\n",
    "            NC_count_all += 1\n",
    "    label_counts.append([C_count, NC_count, NC_count_all])\n",
    "\n",
    "dspeech_sample[['C_counts', 'NC_counts', 'NC_counts_all']] = label_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dspeech_sample.to_csv('data_additionaal/tokenized33000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspeech_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsi = dspeech_sample[(dspeech_sample['C_counts']>10) & (dspeech_sample['NC_counts_all']< dspeech_sample['C_counts'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those_eighty = dspeech_sample[(dspeech_sample['C_counts']>3) & (dspeech_sample['NC_counts']< dspeech_sample['C_counts']) & (dspeech_sample[\"NC_counts_all\"]>= dspeech_sample['C_counts'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsi['speaker_party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in qsi['speech_item_text'].to_list():\n",
    "    print(text)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diw_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
